# src/bot/pandora_generativa.py
"""
Pandora Generativa (ProtÃ³tipo)
VersÃ£o conceitual que usaria um modelo generativo para respostas empÃ¡ticas.
NÃ£o usada no projeto por questÃµes de custo de inferÃªncia.
"""

from typing import Optional

SAFETY_PREFIX = """
VocÃª Ã© a Pandora, uma IA empÃ¡tica.
Nunca dÃª conselhos mÃ©dicos.
Nunca encoraje comportamentos de risco.
Se detectar crise, mostre esta mensagem:

ğŸ’› Sinto muito que vocÃª esteja se sentindo assim.
Eu nÃ£o posso oferecer ajuda de emergÃªncia.
Por favor, ligue 188 (CVV) ou procure atendimento profissional imediatamente.
"""

CRISIS = [
    "me matar", "suicidio", "suicÃ­dio", "tirar minha vida",
    "nÃ£o quero mais viver", "kill myself", "suicide", "want to die"
]


def is_crisis(text: str) -> bool:
    t = text.lower()
    return any(k in t for k in CRISIS)


def generate_with_llm(prompt: str) -> str:
    """
    Mock para chamada generativa.
    Aqui poderia entrar qualquer LLM:
    - OpenAI
    - HuggingFace
    - Mistral
    - Llama.cpp local
    - GPT4All
    """

    # EXEMPLO SIMBÃ“LICO
    # (retorna uma resposta fixa sÃ³ para teste)
    return (
        "Compreendo. Parece que vocÃª estÃ¡ passando por algo importante. "
        "Se quiser, posso te ajudar a explorar seus sentimentos."
    )


def handle_pandora_generativa(user_id: str, text: str) -> str:
    # 1) Safety first
    if is_crisis(text):
        return (
            "ğŸ’› Sinto muito que vocÃª esteja se sentindo assim.\n\n"
            "Eu sou apenas um assistente virtual e **nÃ£o posso oferecer ajuda de emergÃªncia**.\n"
            "No Brasil, ligue **188** (CVV)."
        )

    # 2) Monta prompt seguro
    prompt = SAFETY_PREFIX + "\nUsuÃ¡rio: " + text + "\nPandora:"

    # 3) Chama modelo generativo (placeholder)
    resposta = generate_with_llm(prompt)

    return resposta